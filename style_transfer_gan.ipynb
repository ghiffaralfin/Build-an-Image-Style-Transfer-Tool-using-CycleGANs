{"cells":[{"cell_type":"markdown","id":"3538e246-82fd-4a30-97e1-61fed0195e22","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"51e34a08-4f05-4985-88f5-8281c9f81172","metadata":{},"source":["# Transform Photos to Monet Paintings with CycleGANs\n"]},{"cell_type":"markdown","id":"82be84ca-6b82-4dc3-bf4e-8ea6c2071fb8","metadata":{},"source":["Estimated time needed: **60** minutes\n","\n","Artists like Claude Monet are recognized for the unique styles of their works, such as the unique colour scheme and brush strokes. These are hard to be imitated by normal people, and even for professional painters, it will not be easy to produce a painting whose style is Monet-esque. However, thanks to the invention of Generative Adversarial Networks (GANs) and their many variations, Data Scientists and Machine Learning Engineers can build and train deep learning models to bring an artist's peculiar style to your photos. \n","\n","In this project, you will be guided through building a style transfer tool using CycleGAN to \"translate\" the photos in your album into paintings that are Monet-esque!\n","\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0KSEEN/images/starry_night.png\" width=\"60%\">\n"]},{"cell_type":"markdown","id":"8c15e00a-e22a-423c-b078-0e7db7905e24","metadata":{},"source":["## Table of Contents\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"#Defining Helper Functions\">Defining Helper Functions</a></li>\n","        </ol>       \n","    </li>\n","    <li><a href=\"#What is Image Style Transfer in Deep Learning?\">What is Image Style Transfer in Deep Learning?</a></li>\n","    <li><a href=\"#CycleGANs\">CycleGANs</a>\n","        <ol>\n","            <li><a href=\"#A quick recap on vanilla GANs\">A quick recap on vanilla GANs</a></li>\n","            <li><a href=\"#What's novel about CycleGANs?\">What's novel about CycleGANs?</a></li>\n","        </ol>  \n","    </li>   \n","    <li><a href=\"#Data Loading\">Data Loading</a></li>\n","    <li><a href=\"#Building the Generator\">Building the Generator</a>\n","        <ol>\n","            <li><a href=\"#Defining the Downsampling Block\">Defining the Downsampling Block</a></li>\n","            <li><a href=\"#Defining the Upsampling Block\">Defining the Upsampling Block</a></li>\n","            <li><a href=\"#Assembling the Generator\">Assembling the Generator</a></li>\n","        </ol>  \n","    </li>   \n","    <li><a href=\"#Building the Discriminator\">Building the Discriminator</a></li>\n","    <li><a href=\"#Building the CycleGAN Model\">Building the CycleGAN Model</a>\n","    <li><a href=\"#Defining Loss Functions\">Defining Loss Functions</a> \n","    <li><a href=\"#Model Training\">Model Training</a>  \n","         <ol>\n","            <li><a href=\"#Training the CycleGAN\">Training the CycleGAN</a></li>\n","        </ol>  \n","    </li>     \n","    <li><a href=\"#Visualize our Monet-esque photos\">Visualize our Monet-esque photos</a>\n","        <ol>\n","            <li><a href=\"#Loading the Pre-trained Weights\">Loading the Pre-trained Weights</a></li>\n","            <li><a href=\"#Visualizing Style Transfer Output\">Visualizing Style Transfer Output</a></li>\n","        </ol>       \n","    </li>\n","</ol>\n"]},{"cell_type":"markdown","id":"3ea16c50-6639-4208-a402-ad345d2176e0","metadata":{},"source":["## Objectives\n","\n","After completing this project you will be able to:\n","\n","*   Describe the novelty about CycleGANs\n","*   Understand Cycle Consistency Loss\n","*   Describe the complicated architecture of a CycleGAN\n","*   Gain good practices of training deep learning models\n","*   Implement a pre-trained CycleGAN for image style transfer\n"]},{"cell_type":"markdown","id":"437f6cab-3a01-40bd-b6af-748fbc4abfcb","metadata":{},"source":["## Setup\n","\n","For this project, we will be using the following libraries:\n","\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`Pillow`](https://pillow.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for image processing functions.\n","*   [`tensorflow`](https://www.tensorflow.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and neural network related functions.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","id":"178ef09d-7245-40b2-a6f5-d50fff5cfb62","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Anaconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"]},{"cell_type":"code","execution_count":3,"id":"ea9c814c-9f96-4c4e-b597-91cc7f7ef788","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting numpy==1.22.3\n","  Using cached numpy-1.22.3-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n","Collecting matplotlib==3.5.1\n","  Using cached matplotlib-3.5.1-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n","Collecting tensorflow==2.9.0\n","  Using cached tensorflow-2.9.0-cp310-cp310-win_amd64.whl.metadata (3.0 kB)\n","Collecting skillsnetwork==0.20.6\n","  Using cached skillsnetwork-0.20.6-py3-none-any.whl.metadata (1.8 kB)\n","Collecting cycler>=0.10 (from matplotlib==3.5.1)\n","  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting fonttools>=4.22.0 (from matplotlib==3.5.1)\n","  Using cached fonttools-4.49.0-cp310-cp310-win_amd64.whl.metadata (162 kB)\n","Collecting kiwisolver>=1.0.1 (from matplotlib==3.5.1)\n","  Using cached kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib==3.5.1) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.5.1) (9.3.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib==3.5.1) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib==3.5.1) (2.8.2)\n","Collecting absl-py>=1.0.0 (from tensorflow==2.9.0)\n","  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting astunparse>=1.6.0 (from tensorflow==2.9.0)\n","  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n","Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0)\n","  Using cached flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0)\n","  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting google-pasta>=0.1.1 (from tensorflow==2.9.0)\n","  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n","Collecting h5py>=2.9.0 (from tensorflow==2.9.0)\n","  Using cached h5py-3.10.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n","Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0)\n","  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting libclang>=13.0.0 (from tensorflow==2.9.0)\n","  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n","Collecting opt-einsum>=2.3.2 (from tensorflow==2.9.0)\n","  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.9.0) (4.25.3)\n","Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.9.0) (65.6.3)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow==2.9.0) (1.16.0)\n","Collecting termcolor>=1.1.0 (from tensorflow==2.9.0)\n","  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.9.0) (4.10.0)\n","Collecting wrapt>=1.11.0 (from tensorflow==2.9.0)\n","  Using cached wrapt-1.16.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.9.0)\n","  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow==2.9.0) (1.62.0)\n","Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0)\n","  Using cached tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n","  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0)\n","  Using cached keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: ipython in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from skillsnetwork==0.20.6) (8.5.0)\n","Collecting ipywidgets<8,>=7 (from skillsnetwork==0.20.6)\n","  Using cached ipywidgets-7.8.1-py2.py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from skillsnetwork==0.20.6) (2.28.1)\n","Requirement already satisfied: tqdm<5,>=4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from skillsnetwork==0.20.6) (4.64.1)\n","Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.9.0)\n","  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n","Collecting comm>=0.1.3 (from ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached comm-0.2.1-py3-none-any.whl.metadata (3.7 kB)\n","Collecting ipython-genutils~=0.2.0 (from ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached ipython_genutils-0.2.0-py2.py3-none-any.whl.metadata (755 bytes)\n","Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets<8,>=7->skillsnetwork==0.20.6) (5.5.0)\n","Collecting widgetsnbextension~=3.6.6 (from ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached widgetsnbextension-3.6.6-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting jupyterlab-widgets<3,>=1.0.0 (from ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyterlab_widgets-1.1.7-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: backcall in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (0.2.0)\n","Requirement already satisfied: decorator in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (0.18.1)\n","Requirement already satisfied: matplotlib-inline in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (0.1.6)\n","Requirement already satisfied: pickleshare in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<3.1.0,>3.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (3.0.31)\n","Requirement already satisfied: pygments>=2.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython->skillsnetwork==0.20.6) (2.13.0)\n","Requirement already satisfied: stack-data in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipython->skillsnetwork==0.20.6) (0.5.1)\n","Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython->skillsnetwork==0.20.6) (0.4.5)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->skillsnetwork==0.20.6) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->skillsnetwork==0.20.6) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->skillsnetwork==0.20.6) (1.26.12)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->skillsnetwork==0.20.6) (2022.9.24)\n","Collecting google-auth<3,>=1.6.3 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached google_auth-2.28.2-py2.py3-none-any.whl.metadata (4.7 kB)\n","Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n","Collecting markdown>=2.6.8 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n","Collecting protobuf>=3.9.2 (from tensorflow==2.9.0)\n","  Using cached protobuf-3.19.6-cp310-cp310-win_amd64.whl.metadata (806 bytes)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n","Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0) (3.0.1)\n","Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n","Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n","Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n","Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython->skillsnetwork==0.20.6) (0.8.3)\n","Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit<3.1.0,>3.0.1->ipython->skillsnetwork==0.20.6) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0) (2.1.5)\n","Collecting notebook>=4.4.1 (from widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached notebook-7.1.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: executing in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->skillsnetwork==0.20.6) (1.1.1)\n","Requirement already satisfied: asttokens in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->skillsnetwork==0.20.6) (2.0.8)\n","Requirement already satisfied: pure-eval in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->skillsnetwork==0.20.6) (0.2.2)\n","Collecting jupyter-server<3,>=2.4.0 (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyter_server-2.13.0-py3-none-any.whl.metadata (8.4 kB)\n","Collecting jupyterlab-server<3,>=2.22.1 (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyterlab_server-2.25.3-py3-none-any.whl.metadata (5.9 kB)\n","Collecting jupyterlab<4.2,>=4.1.1 (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyterlab-4.1.4-py3-none-any.whl.metadata (15 kB)\n","Collecting notebook-shim<0.3,>=0.2 (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: tornado>=6.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (6.2)\n","Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n","Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0)\n","  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: anyio>=3.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (4.3.0)\n","Collecting argon2-cffi (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached argon2_cffi-23.1.0-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (3.1.3)\n","Collecting jupyter-client>=7.4.4 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyter_client-8.6.0-py3-none-any.whl.metadata (8.3 kB)\n","Collecting jupyter-core!=5.0.*,>=4.12 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyter_core-5.7.1-py3-none-any.whl.metadata (3.4 kB)\n","Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyter_events-0.9.0-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jupyter-server-terminals (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyter_server_terminals-0.5.2-py3-none-any.whl.metadata (5.6 kB)\n","Collecting nbconvert>=6.4.4 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached nbconvert-7.16.2-py3-none-any.whl.metadata (8.0 kB)\n","Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n","Collecting overrides (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: prometheus-client in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (0.20.0)\n","Collecting pywinpty (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached pywinpty-2.0.13-cp310-none-win_amd64.whl.metadata (5.2 kB)\n","Requirement already satisfied: pyzmq>=24 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (24.0.1)\n","Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached Send2Trash-1.8.2-py3-none-any.whl.metadata (4.0 kB)\n","Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached terminado-0.18.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting traitlets>=4.3.1 (from ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached traitlets-5.14.1-py3-none-any.whl.metadata (10 kB)\n","Collecting websocket-client (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached websocket_client-1.7.0-py3-none-any.whl.metadata (7.9 kB)\n","Collecting async-lru>=1.0.0 (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: httpx>=0.25.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (0.27.0)\n","Requirement already satisfied: ipykernel in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (6.16.1)\n","Collecting jupyter-lsp>=2.0.0 (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyter_lsp-2.2.4-py3-none-any.whl.metadata (1.8 kB)\n","Collecting tomli (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n","Collecting babel>=2.10 (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached Babel-2.14.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached json5-0.9.22-py3-none-any.whl.metadata (29 kB)\n","Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n","Collecting requests<3,>=2 (from skillsnetwork==0.20.6)\n","  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (1.2.0)\n","Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (1.0.4)\n","Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (0.14.0)\n","Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n","Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n","Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached referencing-0.33.0-py3-none-any.whl.metadata (2.7 kB)\n","Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached rpds_py-0.18.0-cp310-none-win_amd64.whl.metadata (4.2 kB)\n","Requirement already satisfied: platformdirs>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (2.5.2)\n","Requirement already satisfied: pywin32>=300 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (304)\n","Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: pyyaml>=5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (6.0.1)\n","Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Collecting beautifulsoup4 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n","Collecting bleach!=5.0.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n","Collecting defusedxml (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting jupyterlab-pygments (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n","Collecting mistune<4,>=2.0.3 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting nbclient>=0.5.0 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached nbclient-0.9.0-py3-none-any.whl.metadata (7.8 kB)\n","Collecting pandocfilters>=1.4.1 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n","Collecting tinycss2 (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached tinycss2-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n","Collecting fastjsonschema (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached fastjsonschema-2.19.1-py3-none-any.whl.metadata (2.1 kB)\n","Collecting argon2-cffi-bindings (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl.metadata (6.7 kB)\n","Requirement already satisfied: debugpy>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (1.6.3)\n","Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (1.5.6)\n","Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (5.9.3)\n","Collecting webencodings (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n","Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n","Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n","Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached webcolors-1.13-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: cffi>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (1.15.1)\n","Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6) (2.21)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n","Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets<8,>=7->skillsnetwork==0.20.6)\n","  Using cached types_python_dateutil-2.8.19.20240106-py3-none-any.whl.metadata (1.8 kB)\n","Using cached numpy-1.22.3-cp310-cp310-win_amd64.whl (14.7 MB)\n","Using cached matplotlib-3.5.1-cp310-cp310-win_amd64.whl (7.2 MB)\n","Using cached tensorflow-2.9.0-cp310-cp310-win_amd64.whl (444.1 MB)\n","Using cached skillsnetwork-0.20.6-py3-none-any.whl (26 kB)\n","Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n","Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Using cached fonttools-4.49.0-cp310-cp310-win_amd64.whl (2.2 MB)\n","Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","Using cached h5py-3.10.0-cp310-cp310-win_amd64.whl (2.7 MB)\n","Using cached ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\n","Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","Using cached kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n","Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n","   ---------------------------------------- 24.4/24.4 MB 1.9 MB/s eta 0:00:00\n","Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n","   ---------------------------------------- 65.5/65.5 kB 272.1 kB/s eta 0:00:00\n","Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","   ---------------------------------------- 5.8/5.8 MB 1.0 MB/s eta 0:00:00\n","Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n","   ---------------------------------------- 895.7/895.7 kB 1.2 MB/s eta 0:00:00\n","Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","   ---------------------------------------- 438.7/438.7 kB 1.6 MB/s eta 0:00:00\n","Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n","   ---------------------------------------- 1.5/1.5 MB 762.0 kB/s eta 0:00:00\n","Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n","Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n","Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n","Downloading google_auth-2.28.2-py2.py3-none-any.whl (186 kB)\n","   ---------------------------------------- 186.9/186.9 kB 1.1 MB/s eta 0:00:00\n","Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n","Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\n","   ---------------------------------------- 295.4/295.4 kB 2.0 MB/s eta 0:00:00\n","Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n","   ---------------------------------------- 103.9/103.9 kB 1.5 MB/s eta 0:00:00\n","Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n","Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n","   ---------------------------------------- 781.3/781.3 kB 2.4 MB/s eta 0:00:00\n","Downloading wheel-0.42.0-py3-none-any.whl (65 kB)\n","   ---------------------------------------- 65.4/65.4 kB 1.8 MB/s eta 0:00:00\n","Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\n","   ---------------------------------------- 1.6/1.6 MB 986.2 kB/s eta 0:00:00\n","Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n","Downloading notebook-7.1.1-py3-none-any.whl (5.0 MB)\n","   ---------------------------------------- 5.0/5.0 MB 2.3 MB/s eta 0:00:00\n","Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n","   ---------------------------------------- 181.3/181.3 kB 1.8 MB/s eta 0:00:00\n","Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n","Downloading rsa-4.9-py3-none-any.whl (34 kB)\n","Downloading jupyter_server-2.13.0-py3-none-any.whl (383 kB)\n","   ---------------------------------------- 383.2/383.2 kB 2.6 MB/s eta 0:00:00\n","Downloading traitlets-5.14.1-py3-none-any.whl (85 kB)\n","   ---------------------------------------- 85.4/85.4 kB 2.4 MB/s eta 0:00:00\n","Downloading jupyterlab-4.1.4-py3-none-any.whl (11.4 MB)\n","   ---------------------------------------- 11.4/11.4 MB 2.6 MB/s eta 0:00:00\n","Downloading jupyterlab_server-2.25.3-py3-none-any.whl (58 kB)\n","   ---------------------------------------- 59.0/59.0 kB 3.0 MB/s eta 0:00:00\n","Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n","Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","   ---------------------------------------- 151.7/151.7 kB 1.3 MB/s eta 0:00:00\n","Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n","   ---------------------------------------- 84.9/84.9 kB 597.5 kB/s eta 0:00:00\n","Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n","Downloading Babel-2.14.0-py3-none-any.whl (11.0 MB)\n","   ---------------------------------------- 11.0/11.0 MB 1.9 MB/s eta 0:00:00\n","Downloading json5-0.9.22-py3-none-any.whl (29 kB)\n","Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n","   ---------------------------------------- 85.5/85.5 kB 4.7 MB/s eta 0:00:00\n","Downloading jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n","   ---------------------------------------- 105.9/105.9 kB 3.0 MB/s eta 0:00:00\n","Downloading jupyter_core-5.7.1-py3-none-any.whl (28 kB)\n","Downloading jupyter_events-0.9.0-py3-none-any.whl (18 kB)\n","Downloading jupyter_lsp-2.2.4-py3-none-any.whl (69 kB)\n","   ---------------------------------------- 69.1/69.1 kB 3.9 MB/s eta 0:00:00\n","Downloading nbconvert-7.16.2-py3-none-any.whl (257 kB)\n","   ---------------------------------------- 257.3/257.3 kB 1.8 MB/s eta 0:00:00\n","Downloading nbformat-5.9.2-py3-none-any.whl (77 kB)\n","   ---------------------------------------- 77.6/77.6 kB 2.2 MB/s eta 0:00:00\n","Downloading Send2Trash-1.8.2-py3-none-any.whl (18 kB)\n","Downloading terminado-0.18.0-py3-none-any.whl (14 kB)\n","Downloading pywinpty-2.0.13-cp310-none-win_amd64.whl (1.4 MB)\n","   ---------------------------------------- 1.4/1.4 MB 2.4 MB/s eta 0:00:00\n","Downloading argon2_cffi-23.1.0-py3-none-any.whl (15 kB)\n","Downloading jupyter_server_terminals-0.5.2-py3-none-any.whl (13 kB)\n","Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n","Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n","   ---------------------------------------- 58.5/58.5 kB 1.6 MB/s eta 0:00:00\n","Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n","   ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n","Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n","   ---------------------------------------- 162.8/162.8 kB 2.4 MB/s eta 0:00:00\n","Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n","Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n","   ---------------------------------------- 48.0/48.0 kB 2.5 MB/s eta 0:00:00\n","Downloading nbclient-0.9.0-py3-none-any.whl (24 kB)\n","Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n","Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n","Downloading referencing-0.33.0-py3-none-any.whl (26 kB)\n","Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Downloading rpds_py-0.18.0-cp310-none-win_amd64.whl (206 kB)\n","   ---------------------------------------- 206.7/206.7 kB 2.1 MB/s eta 0:00:00\n","Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)\n","Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n","   ---------------------------------------- 147.9/147.9 kB 1.8 MB/s eta 0:00:00\n","Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n","Downloading fastjsonschema-2.19.1-py3-none-any.whl (23 kB)\n","Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n","Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n","Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n","Downloading webcolors-1.13-py3-none-any.whl (14 kB)\n","Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n","Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n","Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n","   ---------------------------------------- 66.4/66.4 kB 1.8 MB/s eta 0:00:00\n","Downloading types_python_dateutil-2.8.19.20240106-py3-none-any.whl (9.7 kB)\n","Installing collected packages: webencodings, tensorboard-plugin-wit, libclang, keras, ipython-genutils, flatbuffers, fastjsonschema, wrapt, wheel, websocket-client, webcolors, uri-template, types-python-dateutil, traitlets, tomli, tinycss2, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, soupsieve, send2trash, rpds-py, rfc3986-validator, rfc3339-validator, requests, pywinpty, python-json-logger, pyasn1, protobuf, pandocfilters, overrides, oauthlib, numpy, mistune, markdown, kiwisolver, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, json5, google-pasta, gast, fqdn, fonttools, defusedxml, cycler, cachetools, bleach, babel, attrs, async-lru, absl-py, terminado, rsa, requests-oauthlib, referencing, pyasn1-modules, opt-einsum, matplotlib, keras-preprocessing, jupyter-core, h5py, comm, beautifulsoup4, astunparse, arrow, argon2-cffi-bindings, jupyter-server-terminals, jupyter-client, jsonschema-specifications, isoduration, google-auth, argon2-cffi, jsonschema, google-auth-oauthlib, tensorboard, nbformat, tensorflow, nbclient, jupyter-events, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, widgetsnbextension, ipywidgets, skillsnetwork\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.5.0\n","    Uninstalling traitlets-5.5.0:\n","      Successfully uninstalled traitlets-5.5.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.28.1\n","    Uninstalling requests-2.28.1:\n","      Successfully uninstalled requests-2.28.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.25.3\n","    Uninstalling protobuf-4.25.3:\n","      Successfully uninstalled protobuf-4.25.3\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 22.1.0\n","    Uninstalling attrs-22.1.0:\n","      Successfully uninstalled attrs-22.1.0\n","  Attempting uninstall: jupyter-core\n","    Found existing installation: jupyter_core 4.11.2\n","    Uninstalling jupyter_core-4.11.2:\n","      Successfully uninstalled jupyter_core-4.11.2\n","  Attempting uninstall: jupyter-client\n","    Found existing installation: jupyter_client 7.4.3\n","    Uninstalling jupyter_client-7.4.3:\n","      Successfully uninstalled jupyter_client-7.4.3\n","Successfully installed absl-py-2.1.0 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 arrow-1.3.0 astunparse-1.6.3 async-lru-2.0.4 attrs-23.2.0 babel-2.14.0 beautifulsoup4-4.12.3 bleach-6.1.0 cachetools-5.3.3 comm-0.2.1 cycler-0.12.1 defusedxml-0.7.1 fastjsonschema-2.19.1 flatbuffers-1.12 fonttools-4.49.0 fqdn-1.5.1 gast-0.4.0 google-auth-2.28.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.10.0 ipython-genutils-0.2.0 ipywidgets-7.8.1 isoduration-20.11.0 json5-0.9.22 jsonpointer-2.4 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 jupyter-client-8.6.0 jupyter-core-5.7.1 jupyter-events-0.9.0 jupyter-lsp-2.2.4 jupyter-server-2.13.0 jupyter-server-terminals-0.5.2 jupyterlab-4.1.4 jupyterlab-pygments-0.3.0 jupyterlab-server-2.25.3 jupyterlab-widgets-1.1.7 keras-2.9.0 keras-preprocessing-1.1.2 kiwisolver-1.4.5 libclang-16.0.6 markdown-3.5.2 matplotlib-3.5.1 mistune-3.0.2 nbclient-0.9.0 nbconvert-7.16.2 nbformat-5.9.2 notebook-7.1.1 notebook-shim-0.2.4 numpy-1.22.3 oauthlib-3.2.2 opt-einsum-3.3.0 overrides-7.7.0 pandocfilters-1.5.1 protobuf-3.19.6 pyasn1-0.5.1 pyasn1-modules-0.3.0 python-json-logger-2.0.7 pywinpty-2.0.13 referencing-0.33.0 requests-2.31.0 requests-oauthlib-1.3.1 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 rpds-py-0.18.0 rsa-4.9 send2trash-1.8.2 skillsnetwork-0.20.6 soupsieve-2.5 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 terminado-0.18.0 tinycss2-1.2.1 tomli-2.0.1 traitlets-5.14.1 types-python-dateutil-2.8.19.20240106 uri-template-1.3.0 webcolors-1.13 webencodings-0.5.1 websocket-client-1.7.0 wheel-0.42.0 widgetsnbextension-3.6.6 wrapt-1.16.0\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","grpcio-health-checking 1.62.0 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n","grpcio-reflection 1.62.0 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n","scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 1.22.3 which is incompatible.\n"]}],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","!pip install numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 skillsnetwork==0.20.6\n","\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n","\n","# RESTART YOUR KERNEL AFTERWARD AS WELL"]},{"cell_type":"markdown","id":"9dc729c9-bd49-43db-8c34-371d300f3fdc","metadata":{},"source":["Run the following upgrade and then **RESTART YOUR KERNEL**. Make sure the version of tensorflow imported below is **no less than 2.9.0**.\n"]},{"cell_type":"code","execution_count":null,"id":"73f2e0a7-1934-4d5d-a671-aade39295fac","metadata":{},"outputs":[],"source":[" %%capture\n","!pip3 install --upgrade tensorflow\n","!pip3 install tensorflow_addons"]},{"cell_type":"markdown","id":"dadc1942-5290-4d88-a1fa-2ed06ec1c592","metadata":{},"source":["### Importing Required Libraries\n","\n","*We recommend you import all required libraries in one place (here):*\n"]},{"cell_type":"code","execution_count":null,"id":"b46f7e2d-de41-4c3b-a5ea-2810f88c2c88","metadata":{},"outputs":[],"source":["import warnings\n","warnings.simplefilter('ignore')\n","\n","import keras\n","from keras.utils.vis_utils import plot_model\n","\n","import numpy as np\n","import tensorflow as tf\n","print(f\"tensorflow version: {tf.__version__}\")\n","import tensorflow_addons as tfa\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Input,Conv2DTranspose,BatchNormalization,ReLU,Conv2D,LeakyReLU\n","\n","\n","from IPython import display\n","\n","import skillsnetwork\n","print(f\"skillsnetwork version: {skillsnetwork.__version__}\")\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","\n","import os\n","from os import listdir\n","from pathlib import Path\n","import imghdr\n","\n","import time\n","from tqdm.auto import tqdm\n","import time\n","from PIL import Image\n","import random"]},{"cell_type":"markdown","id":"884763d8-93ba-4c3b-ba17-221243f6ee26","metadata":{},"source":["If you choose to download this notebook and run it in an environment where a TPU accelerator is available, the following code does the initialization work and allows you to implement synchronous distributed training with a TPU `strategy`.\n"]},{"cell_type":"code","execution_count":null,"id":"48ab1f88-969d-4968-b2a9-db3ffafd35b4","metadata":{},"outputs":[],"source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","    print('Device:', tpu.master())\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","except:\n","    strategy = tf.distribute.get_strategy()\n","print('Number of replicas:', strategy.num_replicas_in_sync)\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","    \n","print(tf.__version__)"]},{"cell_type":"markdown","id":"15668eb6-ec2c-4380-855f-dc5dbbde5286","metadata":{},"source":["### Defining Helper Functions\n"]},{"cell_type":"code","execution_count":null,"id":"314696b2-4d68-4359-a0a5-da77bdf70e5f","metadata":{},"outputs":[],"source":["def decode_image(file_path):\n","    \n","    \"\"\"\n","    return a decoded tf tensor given an image file path, \n","    \n","    \"\"\"\n","    image = tf.io.read_file(file_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = (tf.cast(image, tf.float32) / 127.5) - 1\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n","    return image\n","\n","\n","def load_dataset(directory, labeled=True):\n","    \n","    \"\"\"\n","    return a tf.data.Dataset consisting of images from directory, decoded by the function decode_image\n","    \n","    \"\"\"\n","    dataset = tf.data.Dataset.list_files(directory + \"/*.jpg\")\n","    dataset = dataset.map(decode_image, num_parallel_calls=AUTOTUNE)\n","    \n","    return dataset"]},{"cell_type":"markdown","id":"5e548113-665d-4362-a36e-419ef761e267","metadata":{},"source":["## What is Image Style Transfer in Deep learning?\n","\n","Image Style Transfer can be viewed as an image-to-image translation, where we \"translate\" image A into a new image by combining the content of image A with the style of another image B (which could be a famous Monet painting). Consider the following examples of image style transfers from the original [CycleGAN](https://junyanz.github.io/CycleGAN/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01) project, this type of operation is both interesting and powerful as it can translate photos from summer to winter, horse to zebra, and photo to artist's painting. \n","\n","\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0KSEEN/images/style_transfers.png\" width=\"100%\" style=\"vertical-align:middle;margin:20px 0px\"></center>\n","\n","<p style=\"color:gray; text-align:center;\">Image credits to <a href=\"https://junyanz.github.io/CycleGAN/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01\">CycleGAN authors</a></p>\n"]},{"cell_type":"markdown","id":"07c1525a-777a-4f8c-aded-2cc1fa8312d7","metadata":{},"source":["## CycleGANs\n"]},{"cell_type":"markdown","id":"d8693efb-014e-40e8-87d2-e232fa79e268","metadata":{},"source":["### A quick recap on vanilla GANs\n","\n","GANs are a family of algorithms are use _learning by comparison_. A vanilla GAN has two parts, a Generator network which we denote by $\\boldsymbol G$ and a Discriminator network which we denote by $\\boldsymbol D$. $\\boldsymbol G$ tries to fool $\\boldsymbol D$ through continuously improving its own ability to produce images that are fake but close to the real images. $\\boldsymbol D$ is responsible for distinguishing the fake images from the real images and keeps on improving the correctness of its predictions (classifications). Here is a visualization of a vanilla GAN architecture from [this publication](https://www.researchgate.net/figure/Architecture-of-vanilla-GAN_fig1_347477054?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01):\n","\n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0KSEEN/images/architecture-of-vanilla-gan.png\" width=\"90%\" style=\"vertical-align:middle;margin:20px 0px\"></center>\n","\n","\n","\n","<p style=\"color:gray; text-align:center;\">Image credits to <a href=\"https://www.researchgate.net/figure/Architecture-of-vanilla-GAN_fig1_347477054?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01\">Jaeuk Moon et al.</a></p>\n"]},{"cell_type":"markdown","id":"f62c4d0a-5765-4d3e-93dc-601380c3a8c5","metadata":{},"source":["Vanilla GANs use adversarial training to optimize both $\\boldsymbol G$ and $\\boldsymbol D$ at the same time (well, not quite exactly at the same time but at least both are optimized in one iteration!). In adversatial training, we take into account the losses of both networks when designing the objective function.\n"]},{"cell_type":"markdown","id":"6685377f-e681-4ea4-bebb-c78689411eef","metadata":{},"source":["### What's novel about CycleGANs?\n"]},{"cell_type":"markdown","id":"c204da65-8db3-4445-becf-ec6cb2070501","metadata":{},"source":["First of all, unlike other GAN models for image-to-image translation, CycleGANs **do not require paired training data**. For example, if we are interested in translating photographs of winter landscapes to summer landscapes, we do not require each winter landscape to have its corresponding summer view exist in the dataset. This allows the development of image translation models for tasks where paired training datasets are not available.\n","\n","Besides, CycleGAN uses the additional **Cycle Consistency Loss** to enforce the forward-backward consistency of the Generators. \n","\n","<p style=\"color:blue;\">What is forward-backward consistency?</p>\n","\n","As a simple example, if we translate a sentence from English to French, and then translate it back from French to Engligh, we should expect to get back the original english sentence. \n","\n","<p style=\"color:blue;\">Why we need forward-backward consistency?</p>\n","\n","With one $\\boldsymbol G$ that learns a mapping between two domains of images $X$ and $Y$ such that the output $\\hat y = \\boldsymbol G(x)$ is indistinguishable from $y \\in Y$ by $\\boldsymbol D$ trained to classify $\\hat y$ apart from $y$, we simply can not guarantee that $x$ and $y$ are paired up in a meaningful way. For instance, it probably won't make sense to transfer the style of a Fuji mountain photo in the same way we transfer the style of a cat photo.\n","\n","Hence, to ensure that the mapping $\\boldsymbol G: X \\rightarrow Y$ is constrained and meaningful, we introduce a second, inverse mapping $\\boldsymbol F: Y \\rightarrow X$ to ensure that $F(G(x)) \\approx x$ and $G(F(y)) \\approx y$, which means **an image translation cycle should be able to bring an image back to the original image**.\n","\n","The loss that incurred during the image translation cycle, i.e., the discrepancy between $x$ and  $F(G(x))$, $y$ and $G(F(y))$, will be added to the objective function of a CycleGAN as the **Cycle Consistency Loss**.\n"]},{"cell_type":"markdown","id":"2493b365-1598-418b-aef8-c8dc7d23eb04","metadata":{},"source":["We will get into the architecture of CycleGANs later in this lab when we start building the networks. For now, you know how CycleGANs are different from vanilla GANs in terms of the objective function used, let's kick-start our CycleGAN project by loading in some unpaired data first!\n"]},{"cell_type":"markdown","id":"554be1d2-5281-43e6-8546-b96de9750630","metadata":{},"source":["## Data Loading\n"]},{"cell_type":"markdown","id":"8f646c76-6ec6-4efd-8b67-421c62761ea5","metadata":{},"source":["The unpaired dataset comes from a Kaggle competition called [I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started/data?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01). The original dataset contains around 400MB of images, but for this project we will only use 300 Monet paintings and 300 photos for training the CycleGAN. The followinig cell downloads the zipped dataset.\n"]},{"cell_type":"code","execution_count":null,"id":"506df5cd-2e04-4bd3-88a7-3dddc0260307","metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0KSEEN/data/gan_getting_started_300.zip\", overwrite=True)"]},{"cell_type":"markdown","id":"8f3b4538-6b22-4e3e-8c70-36d3dc9ad37c","metadata":{},"source":["In your current working directory, there is a folder called `gan_getting_started_300` which contains two subfolders: `monet_jpg_300` and `photo_jpg_300`. We confirm that each of the subfolders contains 300 JPG format images:\n"]},{"cell_type":"code","execution_count":null,"id":"5281cdce-c4d4-4605-8877-900a43021d33","metadata":{},"outputs":[],"source":["IMG_PATH = \"gan_getting_started_300\"\n","\n","MONET_FILENAMES = tf.io.gfile.glob(str(IMG_PATH + '/monet_jpg_300/*.jpg'))\n","print('Monet JPG Files:', len(MONET_FILENAMES))\n","\n","PHOTO_FILENAMES = tf.io.gfile.glob(str(IMG_PATH + '/photo_jpg_300/*.jpg'))\n","print('Photo JPG Files:', len(PHOTO_FILENAMES))"]},{"cell_type":"markdown","id":"faf4a9b0-80f6-4000-90f5-423579eedb05","metadata":{},"source":["The `load_dataset` helper function utilizes the `Dataset` API from `tf.data`, which allows you to build asynchronous, highly optimized data pipelines. The `.batch(n)` combines consecutive elements of a dataset into batches. We create two `tf.data.Dataset` objects for feeding our Monet painting and photo data into the CycleGAN in a streaming fashion.\n"]},{"cell_type":"code","execution_count":null,"id":"4d054f30-5c76-4439-b19e-d964fac68547","metadata":{},"outputs":[],"source":["IMAGE_SIZE = [256, 256]\n","\n","monet_ds = load_dataset(\"gan_getting_started_300/monet_jpg_300\", labeled=True).batch(1)\n","photo_ds = load_dataset(\"gan_getting_started_300/photo_jpg_300\", labeled=True).batch(1)\n","\n","monet_ds, photo_ds"]},{"cell_type":"markdown","id":"b5f5c28f-1092-412c-8b92-4593f5de7640","metadata":{},"source":["Let's visualize a photo example and a Monet example:\n"]},{"cell_type":"code","execution_count":null,"id":"68df3b5c-8883-45e6-8230-42cb4641f597","metadata":{},"outputs":[],"source":["example_monet = next(iter(monet_ds))\n","example_photo = next(iter(photo_ds))"]},{"cell_type":"code","execution_count":null,"id":"e485ae46-554b-46c4-ba3b-02342e92877c","metadata":{},"outputs":[],"source":["plt.subplot(121)\n","plt.title('Photo')\n","plt.imshow(example_photo[0] * 0.5 + 0.5)\n","\n","plt.subplot(122)\n","plt.title('Monet')\n","plt.imshow(example_monet[0] * 0.5 + 0.5)"]},{"cell_type":"markdown","id":"35b229f9-8a32-4764-b514-13cf30a6e9de","metadata":{},"source":["You can find various implementations of CycleGANs in this [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). For this project, we will borrow a simpler implementation of CycleGANs for image style transfer. Thanks to Amy Jang, who created an easy-to-understand tutorial that goes over the basics of loading data from TFRecords, using TPUs, and building a CycleGAN for the [I'm Something of a Painter Myself](https://www.kaggle.com/competitions/gan-getting-started/data?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01) Kaggle competition. You can find the original tutorial [here](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01). \n"]},{"cell_type":"markdown","id":"6757390d-f345-4fce-af5e-386ee103db0a","metadata":{},"source":["## Building the Generator \n"]},{"cell_type":"markdown","id":"0b3e8fd3-8fe5-456d-9867-90b5ff0ff51d","metadata":{},"source":["### Defining the Downsampling Block\n"]},{"cell_type":"markdown","id":"6b1c1b66-de2d-4c34-8f36-6e746980f138","metadata":{},"source":["The CycleGAN Generator model takes an input image and generates an output image. To achieve this, the model architecture begins with a sequence of **downsampling convolutional blocks** (reduce the 2D dimensions, width and height of an image by the stride) to encode the input image. \n","\n","To define a downsampling block, we will use the instance normalization method instead of batch normalization as our batch size is very small. **InstanceNorm transforms each training sample independently over multiple channels, whereas BatchNorm does that to the whole batch of samples over each channel**. The intent is to remove image-specific contrast information from the image, which simplifies the generation and results in better generated images.\n"]},{"cell_type":"markdown","id":"361051f1-790d-4b56-aeef-7ca543cd4322","metadata":{},"source":["As the instance normalization is not standard in the TensorFlow API, we'll use the layer from TensorFlow Add-ons.\n"]},{"cell_type":"code","execution_count":null,"id":"1329525b-2c92-4e3d-af17-3bcc8132550d","metadata":{},"outputs":[],"source":["OUTPUT_CHANNELS = 3\n","\n","def downsample(filters, size, apply_instancenorm=True):\n","    \n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    result = keras.Sequential()\n","    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer=initializer, use_bias=False))\n","\n","    if apply_instancenorm:\n","        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n","\n","    result.add(layers.LeakyReLU())\n","\n","    return result"]},{"cell_type":"markdown","id":"75819700-5c59-4e55-8c90-98812ff9024d","metadata":{},"source":["The defined downsampling block provides an efficient way for us to add multiple **Convolution-InstanceNorm-LeakyReLU** layers with variable number of filters and filter sizes.\n"]},{"cell_type":"markdown","id":"bb147a02-3a22-406d-a3e6-5b4cb6f8c609","metadata":{},"source":["### Defining the Upsampling Block\n"]},{"cell_type":"markdown","id":"365dfd87-4e5d-4d28-bd06-c6cc0786b60b","metadata":{},"source":["Next, the Generator uses a number of **Upsampling blocks** to generate the output image. \n","\n","Upsampling does the opposite of downsampling and increases the dimensions of the image. Hence, we use the `Conv2DTranspose` API from keras to create **TransposeConvolution-InstanceNorm-ReLU** layers to build the block.\n"]},{"cell_type":"code","execution_count":null,"id":"dc97cec2-642b-4cd4-8729-064f330e9593","metadata":{},"outputs":[],"source":["def upsample(filters, size, apply_dropout=False):\n","    \n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    result = keras.Sequential()\n","    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n","                                      padding='same',\n","                                      kernel_initializer=initializer,\n","                                      use_bias=False))\n","\n","    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n","\n","    if apply_dropout:\n","        result.add(layers.Dropout(0.5))\n","\n","    result.add(layers.ReLU())\n","\n","    return result"]},{"cell_type":"markdown","id":"36f525b3-4af2-4943-a028-95607c77bda5","metadata":{},"source":["### Assembling the Generator\n"]},{"cell_type":"markdown","id":"6990e914-8749-41c1-8983-cc81ebec2bd3","metadata":{},"source":["The Generator uses a sequence of **downsampling convolutional blocks** to encode the input image, a number of **residual network (ResNet) convolutional blocks** to transform the image, and a number of **upsampling convolutional blocks** to generate the output image.\n","\n","The ResNet blocks essentially **skip connections to help bypass the vanishing gradient problem** through concatenating the output of downsampling layers directly to the output of upsampling layers. You will see that we concatenate them in a symmetrical fashion in the following code cell.\n","\n","Let's build the Generator model.\n"]},{"cell_type":"code","execution_count":null,"id":"a63d67e1-c369-4ece-ad61-c88327bc35ac","metadata":{},"outputs":[],"source":["def Generator():\n","    inputs = layers.Input(shape=[256,256,3])\n","\n","    # bs = batch size\n","    down_stack = [\n","        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n","        downsample(128, 4), # (bs, 64, 64, 128)\n","        downsample(256, 4), # (bs, 32, 32, 256)\n","        downsample(512, 4), # (bs, 16, 16, 512)\n","        downsample(512, 4), # (bs, 8, 8, 512)\n","        downsample(512, 4), # (bs, 4, 4, 512)\n","        downsample(512, 4), # (bs, 2, 2, 512)\n","        downsample(512, 4), # (bs, 1, 1, 512)\n","    ]\n","\n","    up_stack = [\n","        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n","        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n","        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n","        upsample(512, 4), # (bs, 16, 16, 1024)\n","        upsample(256, 4), # (bs, 32, 32, 512)\n","        upsample(128, 4), # (bs, 64, 64, 256)\n","        upsample(64, 4), # (bs, 128, 128, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n","                                  strides=2,\n","                                  padding='same',\n","                                  kernel_initializer=initializer,\n","                                  activation='tanh') # (bs, 256, 256, 3)\n","\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","\n","    return keras.Model(inputs=inputs, outputs=x)"]},{"cell_type":"markdown","id":"650d17d8-76ee-4ff4-b6a7-2cb7135971f9","metadata":{},"source":["Let's visualize our Generator model's architecture using the `plot_model` function from `keras.utils.vis_untils`.\n"]},{"cell_type":"code","execution_count":null,"id":"d611308d-3bb1-452b-8e25-5ccefd5e9d8b","metadata":{},"outputs":[],"source":["gen = Generator()\n","plot_model(gen, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","id":"87643cc1-d7b5-4155-bd78-9e9572687e09","metadata":{},"source":["By looking at the visualization, you can see very clealy how the skip connections operation works and that all the concatenations are done symmetrically.\n","\n","Also, note that it's a fairly complicated architecture and one Generator alone has 54M parameters to train!\n"]},{"cell_type":"markdown","id":"5039c2ac-110d-455f-aaa7-eb2675d19788","metadata":{},"source":["## Building the Discriminator\n"]},{"cell_type":"markdown","id":"e1a98eb4-b151-4514-8b09-b861dc53a04d","metadata":{},"source":["The discriminator model takes a $256 \\times 256$ color image and is responsible for classifying it as real or fake. \"Fake\" as in being produced by the Generator. \n","\n","This can be implemented directly by borrowing the architecture of a somewhat standard deep convolutional discriminator model. Thus, it can be built by mainly using the **Convolution-InstanceNorm-LeakyReLU** layers. \n","\n","Although the InstanceNorm method was designed for generator models, it can also prove effective in discriminator models. \n","\n","Our Discriminator model will be built using 4 Convolution-InstanceNorm-LeakyReLU layers with 64, 128, 256, and 512 sized 4 filters, respectively. After the last layer, we apply a convolution to produce a 1-dimensional output.\n"]},{"cell_type":"code","execution_count":null,"id":"e21b99a0-f3dc-40e7-9d5f-4c61049787ea","metadata":{},"outputs":[],"source":["def Discriminator():\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n","\n","    x = inp\n","\n","    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n","    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n","    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n","\n","    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n","    conv = layers.Conv2D(512, 4, strides=1,\n","                         kernel_initializer=initializer,\n","                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n","\n","    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n","\n","    leaky_relu = layers.LeakyReLU()(norm1)\n","\n","    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n","\n","    last = layers.Conv2D(1, 4, strides=1,\n","                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n","\n","    return tf.keras.Model(inputs=inp, outputs=last)"]},{"cell_type":"markdown","id":"9a8e3753-e215-4685-805b-219ba3e9c056","metadata":{},"source":["Similarly, we can visualize the architecture of the Discriminator model:\n"]},{"cell_type":"code","execution_count":null,"id":"3812e88e-533b-407a-ad41-4ab4e63ed398","metadata":{},"outputs":[],"source":["disc = Discriminator()\n","plot_model(disc, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","id":"a608dc45-3ed1-41fb-a6aa-4d8ed01fad60","metadata":{},"source":["The discrimiator model looks much less complicated than the generator as it's mainly doing downsampling. But still, if you run `disc.summary()`, you will notice that it has 2M parameters to train, which is not an easy task either.\n","\n","Now that we have the generator and discriminator network ready, it's time to build our mighty CycleGAN model!\n"]},{"cell_type":"markdown","id":"90301c71-a7ef-4c35-b8b1-b74e3053c30f","metadata":{},"source":["## Building the CycleGAN Model\n"]},{"cell_type":"markdown","id":"1467c7ad-fa30-486a-95f6-07ef46d28ef7","metadata":{},"source":["I hope you still remember that in order to ensure that the mapping between images from two domains is meaningful and desirable, we enforce forward-backward consistency by involving two mappings: $\\boldsymbol G: X \\rightarrow Y$ and the inverse $\\boldsymbol F: Y \\rightarrow X$.\n","\n","This means, our CycleGAN model needs two generators. One for transforming photos to Monet-esque paintings and one for transforming Monet paintings to be more like photos.\n","\n","Since we have two generators, we would naturally need two discriminators to \"discriminate\" the work of each of them. This leads to our definition of `monet_generator`, `photo_generator`, `monet_discriminator`, and `photo_discriminator` in the next cell.\n"]},{"cell_type":"code","execution_count":null,"id":"f3b733bc-6545-4a28-89d3-fda681e319c8","metadata":{},"outputs":[],"source":["with strategy.scope():\n","    \n","    monet_generator = Generator() # transforms photos to Monet-esque paintings\n","    photo_generator = Generator() # transforms Monet paintings to be more like photos\n","\n","    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n","    photo_discriminator = Discriminator() # differentiates real photos and generated photos"]},{"cell_type":"markdown","id":"9e86ab9f-c163-417d-b4c7-ce2a71b48cc7","metadata":{},"source":["All 4 networks that we defined here will be used to compile a CycleGAN.\n"]},{"cell_type":"markdown","id":"8e20a735-5319-462d-b905-a89fea2f76ed","metadata":{},"source":["In the next cell, we create the CycleGAN class. Don't panick, you are not required to write the class on your own from scratch, but do take a close look at the code and the comments to understand what each step does, especially for the `train_step` function. \n","\n","Let me give you a brief overview of what's going on in the next cell:\n","\n","- The `CycleGan` class subclasses `tf.keras.Model` so that it retains all the methods of keras models such as the `.fit()` method.\n","- A CycleGan model will compile with 4 optimizers and 4 loss functions, each optimizer is associated with a network.\n","- During a `train_step`, the CycleGan model transforms a real photo to a Monet painting and then back to a photo, a real Monet painting to a photo and then back to Monet painting.\n","- The cycle consistency loss is calculated as the sum of the absolute differences between the two real images and their corresponding cycled images.\n"]},{"cell_type":"code","execution_count":null,"id":"63af69ab-e817-4802-86f6-1d2ba71065ab","metadata":{},"outputs":[],"source":["class CycleGan(keras.Model):\n","    \n","    def __init__(\n","        self,\n","        monet_generator,\n","        photo_generator,\n","        monet_discriminator,\n","        photo_discriminator,\n","        lambda_cycle=10,\n","    ):\n","        super(CycleGan, self).__init__()\n","        self.m_gen = monet_generator\n","        self.p_gen = photo_generator\n","        self.m_disc = monet_discriminator\n","        self.p_disc = photo_discriminator\n","        self.lambda_cycle = lambda_cycle\n","        \n","    def compile(\n","        self,\n","        m_gen_optimizer,\n","        p_gen_optimizer,\n","        m_disc_optimizer,\n","        p_disc_optimizer,\n","        gen_loss_fn,\n","        disc_loss_fn,\n","        cycle_loss_fn,\n","        identity_loss_fn\n","    ):\n","        super(CycleGan, self).compile()\n","        self.m_gen_optimizer = m_gen_optimizer\n","        self.p_gen_optimizer = p_gen_optimizer\n","        self.m_disc_optimizer = m_disc_optimizer\n","        self.p_disc_optimizer = p_disc_optimizer\n","        self.gen_loss_fn = gen_loss_fn\n","        self.disc_loss_fn = disc_loss_fn\n","        self.cycle_loss_fn = cycle_loss_fn\n","        self.identity_loss_fn = identity_loss_fn\n","        \n","        \n","    @tf.function\n","    def train_step(self, batch_data):\n","        real_monet, real_photo = batch_data\n","        \n","        with tf.GradientTape(persistent=True) as tape:\n","            # photo to monet back to photo\n","            fake_monet = self.m_gen(real_photo, training=True)\n","            cycled_photo = self.p_gen(fake_monet, training=True)\n","\n","            # monet to photo back to monet\n","            fake_photo = self.p_gen(real_monet, training=True)\n","            cycled_monet = self.m_gen(fake_photo, training=True)\n","\n","            # generating itself\n","            same_monet = self.m_gen(real_monet, training=True)\n","            same_photo = self.p_gen(real_photo, training=True)\n","\n","            # discriminator used to check, inputing real images\n","            disc_real_monet = self.m_disc(real_monet, training=True)\n","            disc_real_photo = self.p_disc(real_photo, training=True)\n","\n","            # discriminator used to check, inputing fake images\n","            disc_fake_monet = self.m_disc(fake_monet, training=True)\n","            disc_fake_photo = self.p_disc(fake_photo, training=True)\n","\n","            # evaluates generator loss\n","            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n","            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n","\n","            # evaluates total cycle consistency loss\n","            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n","\n","            # evaluates total generator loss\n","            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n","            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n","\n","            # evaluates discriminator loss\n","            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n","            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n","\n","        # Calculate the gradients for generator and discriminator\n","        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n","                                                  self.m_gen.trainable_variables)\n","        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n","                                                  self.p_gen.trainable_variables)\n","\n","        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n","                                                      self.m_disc.trainable_variables)\n","        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n","                                                      self.p_disc.trainable_variables)\n","\n","        # Apply the gradients to the optimizer\n","        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n","                                                 self.m_gen.trainable_variables))\n","\n","        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n","                                                 self.p_gen.trainable_variables))\n","\n","        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n","                                                  self.m_disc.trainable_variables))\n","\n","        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n","                                                  self.p_disc.trainable_variables))\n","        \n","        return {\n","            \"monet_gen_loss\": total_monet_gen_loss,\n","            \"photo_gen_loss\": total_photo_gen_loss,\n","            \"monet_disc_loss\": monet_disc_loss,\n","            \"photo_disc_loss\": photo_disc_loss\n","        }"]},{"cell_type":"markdown","id":"dedf5e60-8552-473c-a493-eccc8c636f4d","metadata":{},"source":["If you wonder what `@tf.function` does, here is the [documentation](https://www.tensorflow.org/api_docs/python/tf/function?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01) from tensorflow. In a nutshell, the decorator `@tf.function`, creates a callable tensorflow computation graph of the function or program that it's placed on top of. The graph stores the flow of information and operations between tensors, thus they can speed up training by eliminating the need for repetitive initializations and computations of variables.\n"]},{"cell_type":"markdown","id":"c9268034-329d-4477-bd80-9d69b8eab4d3","metadata":{},"source":["## Defining Loss Functions\n"]},{"cell_type":"markdown","id":"6c235344-8a70-45c9-9cca-331384abadf0","metadata":{},"source":["A perfect discriminator shoud output all 1s for a real image and all 0s for a fake image. Hence, the `discriminator_loss` compares the discriminator's prediction for a real image to a matrix of 1s and the prediction for a fake image to a matrix of 0s. The differences are quantified using Binary Cross Entropy.\n","\n","The [CycleGAN paper](https://arxiv.org/abs/1703.10593?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01) suggests dividing the loss for the discriminator by half during training, in an effort to slow down updates to the discriminator relative to the generator.\n"]},{"cell_type":"code","execution_count":null,"id":"a105bb27-acbb-4d66-baf6-f524b7abfc97","metadata":{},"outputs":[],"source":["with strategy.scope():\n","    \n","    def discriminator_loss(real, generated):\n","        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n","\n","        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n","\n","        total_disc_loss = real_loss + generated_loss\n","\n","        return total_disc_loss * 0.5"]},{"cell_type":"markdown","id":"6d60748b-33f6-4080-a615-288b2e0514f6","metadata":{},"source":["A perfect generator should fool the discriminator by generating images that the discriminator predicts as real or all 1s. Thus, the `generator_loss` compares the discriminator's prediction for a generated image to a matrix of 1s.\n"]},{"cell_type":"code","execution_count":null,"id":"a287ddb1-ee51-49b0-9f71-c014fd3939d9","metadata":{},"outputs":[],"source":["with strategy.scope():\n","    \n","    def generator_loss(generated):\n","        return tf.keras.losses.BinaryCrossentropy(from_logits=True, \n","                                    reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated),generated)"]},{"cell_type":"markdown","id":"5ae223d5-c5b2-4ac0-91dd-2bcdfa4f1a35","metadata":{},"source":["Here is the `calc_cycle_loss`, or the Cycle Consistency Loss, which is the average of the absolute differences between a real image and a cycled (twice transformed) image.\n"]},{"cell_type":"code","execution_count":null,"id":"7ca67891-de66-4fff-8de7-0e98ab7cc40d","metadata":{},"outputs":[],"source":["#We want our original photo and the twice transformed photo to be similar to one another. \n","#Thus, we can calculate the cycle consistency loss be finding the average of their difference.\n","\n","with strategy.scope():\n","    \n","    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n","        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n","\n","        return LAMBDA * loss1"]},{"cell_type":"markdown","id":"14a1c185-6035-452d-88bf-df57259c5992","metadata":{},"source":["Lastly, we have the `identity_loss`. It penalizes large difference between a real image and an image generated by the Generator based on the real image, as a perfect Generator should not have disparate input and output.\n"]},{"cell_type":"code","execution_count":null,"id":"547119f5-0197-4daa-9262-e00d3f299a92","metadata":{},"outputs":[],"source":["with strategy.scope():\n","    \n","    def identity_loss(real_image, same_image, LAMBDA):\n","        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n","        return LAMBDA * 0.5 * loss"]},{"cell_type":"markdown","id":"71d5dc0c-76da-4c9a-9243-651aaeb3d312","metadata":{},"source":["## Model Training\n"]},{"cell_type":"markdown","id":"d41286f7-3a3a-4335-a0f5-d0d7e84aaed8","metadata":{},"source":["In this section, we will compile and train a CycleGAN. Since our networks have too many parameters (one generator alone has 54M and a discriminator has 2M) and Skills Network Labs currently doesn't have any GPUs available, it will take hours to train a sufficient number of epochs with a CPU such that the model can do a decent job in transfering image styles.\n","\n","Therefore, we will train the model for **one epoch** in this lab, which includes 300 iterations. However, you can also choose to run this notebook locally where you can use GPUs or TPUs, then you are free to increase the number of epochs!\n"]},{"cell_type":"markdown","id":"2282e4d4-21ad-4375-a6eb-ef8e71f8b8f8","metadata":{},"source":["### Training the CycleGAN\n"]},{"cell_type":"markdown","id":"03132c15-cd4f-4de8-837d-cfe73d2aacbb","metadata":{},"source":["As I mentiond before, a CycleGAN is compiled with 4 loss functions and 4 optimizers. Let's define the optimizer for each network and then compile the model.\n"]},{"cell_type":"code","execution_count":null,"id":"9e5431b3-7cac-4d8d-ba03-b284848749eb","metadata":{},"outputs":[],"source":["with strategy.scope():\n","    \n","    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"]},{"cell_type":"code","execution_count":null,"id":"17082d8a-c89e-4faf-81b9-b8dc792f0600","metadata":{},"outputs":[],"source":["with strategy.scope():\n","    \n","    cycle_gan_model = CycleGan(\n","        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n","    )\n","\n","    cycle_gan_model.compile(\n","        m_gen_optimizer = monet_generator_optimizer,\n","        p_gen_optimizer = photo_generator_optimizer,\n","        m_disc_optimizer = monet_discriminator_optimizer,\n","        p_disc_optimizer = photo_discriminator_optimizer,\n","        gen_loss_fn = generator_loss,\n","        disc_loss_fn = discriminator_loss,\n","        cycle_loss_fn = calc_cycle_loss,\n","        identity_loss_fn = identity_loss\n","    )"]},{"cell_type":"markdown","id":"9101629b-ad7c-4625-8287-bd0f9574c8ba","metadata":{},"source":["You may remove the \"#\" signs below to train the model! (Since this is a fairly large model and the Skills Network Labs environment currently only has CPUs, training the large model might cause the issue of a dead kernel. If you do encounter a dead kernel, please restart the kernel and run the above cells again.) **You may also consider to skip the cell below as we are going to provide you with a pre-trained model in the next section!**\n"]},{"cell_type":"code","execution_count":null,"id":"7652d2c2-a8c5-4e36-99de-8db0f0f27e09","metadata":{},"outputs":[],"source":["# history = cycle_gan_model.fit(\n","#     tf.data.Dataset.zip((monet_ds, photo_ds)),\n","#     epochs=1)"]},{"cell_type":"markdown","id":"cb16de3d-bf2e-4bd6-bae3-8875a9971ad1","metadata":{},"source":["Now that you had a taste of training the mighty CycleGAN, remember that our original task is to transform photos to Monet-esque paintings and we would need a fully-trained `monet_generator` to do the job.\n","\n","To save you from having to wait for hours in the Skills Network Labs environment, we had trained a CycleGAN beforehand with all the same configurations we had above for 25 epochs. Now we just have to import the weights of the **Monet Generator network** of the pre-trained CycleGAN to help us complete the Style Transfer task!\n"]},{"cell_type":"markdown","id":"eb07b792-1a1b-4b3a-9a80-6e4a2a642f63","metadata":{},"source":["## Visualize our Monet-esque photos\n"]},{"cell_type":"markdown","id":"e955f19a-ed2b-49fb-80df-df3c4010f186","metadata":{},"source":["### Loading Pre-trained Weights\n"]},{"cell_type":"markdown","id":"5bf2ac8c-3fb1-494f-b0eb-e4f12878d579","metadata":{},"source":["Before we load the trained `monet_generator`, we present to you a screenshot of the CycleGAN's training history. Even though only the monet generator is needed for the style transfer task, but you can see that the losses of all 4 networks: monet generator, monet discriminator, photo generator, and photo discriminated, were being optimized. \n"]},{"cell_type":"markdown","id":"bffbc65e-435f-4a1b-bbe3-6d9a5bf9f923","metadata":{},"source":["<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0KSEEN/images/training_history.png\" style=\"vertical-align:middle;margin:20px 0px\"></center>\n"]},{"cell_type":"markdown","id":"0b7e504c-cd05-47bc-8f4a-f56b0c89b0a2","metadata":{},"source":["Downloading the weights of `monet_generator`:\n"]},{"cell_type":"code","execution_count":null,"id":"41051761-dc4d-4aca-9994-f6bc3516b743","metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/LargeData/monet_generator_50_epochs.tgz\",\n","                           overwrite=True)"]},{"cell_type":"markdown","id":"332b8aa2-2362-4a25-834a-e38d39386406","metadata":{},"source":["Creating the `monet_generator_model` using the `load_model` API from Keras:\n"]},{"cell_type":"code","execution_count":null,"id":"65b40571-60cb-4c98-a798-5e92aefcdfd4","metadata":{},"outputs":[],"source":["# Ignore the warnings of this cell, as we don't need the compilation info to resume training\n","\n","monet_generator_model = tf.keras.models.load_model(\"monet_generator_50_epochs\")"]},{"cell_type":"markdown","id":"c7acd451-d3b6-4fa9-9b6b-6576e5025617","metadata":{},"source":["### Visualizing Style Transfer Output\n"]},{"cell_type":"markdown","id":"3335b65d-7ace-43e2-a6fb-9b72b82562ad","metadata":{},"source":["Finally! We will visualize the style transfer output produced by `monet_generator_model`. We take 5 sample images that are photos of beautiful landscapes in the original dataset and feed them to the model. \n"]},{"cell_type":"code","execution_count":null,"id":"ccaca5ad-8e2c-42d7-b2e1-eba872388e7b","metadata":{},"outputs":[],"source":["for i in range(5):\n","    \n","    # randomly draw a photo from PHOTO_FILENAMES\n","    rand_ind = random.randint(0,299)\n","    img_path = PHOTO_FILENAMES[rand_ind]\n","    img = decode_image(img_path)\n","    img = tf.expand_dims(img, axis=0)\n","    \n","    prediction = monet_generator_model(img, training=False)[0].numpy()\n","    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n","    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n","\n","    fig = plt.figure()\n","    fig.add_subplot(1,2,1)\n","    if i == 0:\n","        plt.title(\"Input Photos\")\n","    plt.imshow(img)\n","    plt.axis(\"off\")\n","    \n","    fig.add_subplot(1,2,2)\n","    if i == 0:\n","        plt.title(\"Monet-esque Paintings\")\n","    plt.imshow(prediction)\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","id":"734a47ae-3369-418f-b97e-cb914ec42d26","metadata":{},"source":["## Congratulations! You have completed this guided project.\n"]},{"cell_type":"markdown","id":"458e85eb-25db-4375-8f7a-6bf48fd5f1a9","metadata":{},"source":["## Author(s)\n"]},{"cell_type":"markdown","id":"a2d952af-37ff-446a-b474-f91b1478decc","metadata":{},"source":["[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0KSEEN39299759-2022-01-01) is a Data Science intern at IBM Skills Network, entering level-5 study in the Mathematics & Statistics undergraduate Coop program at McMaster University.\n"]},{"cell_type":"markdown","id":"6c12d221-c1c2-4869-889b-eba0f195b949","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","id":"5f0646a5-22c5-4cde-bae3-51e7250b46d7","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n","| ----------------- | ------- | ----------- | ------------------ |\n","| 2022-09-14        | 0.1     | Roxanne Li  | Created lab       |\n"]},{"cell_type":"markdown","id":"3c8d575d-aeb9-4690-82c6-a01b2459e7aa","metadata":{},"source":["Copyright  2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
